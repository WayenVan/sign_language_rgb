hydra:
  run: 
    dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}/${hydra.job.name}_${description}
  
description: x3d_with_flownet

csi: csi_sign_language
transform: torchvision.transforms
T: torchvision.transforms
CT: csi_sign_language.data.transforms

phoenix14_root: preprocessed/ph14_lmdb

seed: 3407
device: cuda
epoch: 80
#-1 when we don want message out
message_interval: 100
non_block: False
pin_memory: False
num_workers: 8
batch_size: 2
vocab_size: 1296

#if continuos read model
load_weights: True
is_resume: False
checkpoint: outputs/train/2024-03-11_17-13-04/train_x3d_with_flownet/checkpoint.pt

model:
  _target_: ${csi}.models.slr_ctc_baseline.SLRModel
  loss_temp: 8
  loss_weight: [1.0, 0., 0.]
  backbone: 
    _target_: ${csi}.modules.slr_base.base_stream.BaseStream
    encoder: 
      _target_: ${csi}.modules.slr_base.encoders.X3DFlowEncoder
      color_range: [-1., 1.]
      out_channels: 1024
      flownet_checkpoint: resources/FlowNet2-SD_checkpoint.pth.tar
      freeze_flownet: True
      fusion_layers:
        - [24, 24]
        - [24, 24]
        - [48, 48]
        - [96, 48]
        - [192, 48]
    decoder:
      _target_: ${csi}.modules.slr_base.decoders.TransformerDecoder
      n_class: ${vocab_size}
      d_model: 1024
      n_heads: 8
      n_layers: 6
      d_feedforward: 2048

optimizer:
  _target_: torch.optim.Adam
  lr: 5e-5
  weight_decay: 0.0001

lr_scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  step_size: 20
  gamma: 0.8

trainner:
  _target_: ${csi}.engines.trainner.Trainner
  device: ${device}
  message_interval: ${message_interval}

inferencer:
  _target_: ${csi}.engines.inferencer.Inferencer
  device: ${device}

data:
  excluded: ["13April_2011_Wednesday_tagesschau_default-14",]
  subset: multisigner
  collate_fn: 
    _target_: ${csi}.data.dataset.phoenix14.CollateFn

  transform_train: 
        _target_: ${T}.Compose
        transforms: 
        - _target_: ${CT}.video.RandomBrightJitter
          prob: 1
          factor_range: [0.8, 1.2]
        - _target_: ${CT}.video.RandomHorizontalFlip
          prob: 0.5
        - _target_: ${CT}.video.RandomRotate
          prob: 0.7
          angle_range: [-90, 90]
        - _target_: ${CT}.video.RandomCrop
          size: 224
        - _target_: ${CT}.video.Resize
          h: 192
          w: 192
        - _target_: ${CT}.video.FrameScale
          min: -1.
          max: 1.
          input_range: [0, 255]
        - _target_: ${CT}.video.ToTensor
          keys: [video, gloss]
          dtypes: [float32, default]

  transform_val: 
        _target_: ${T}.Compose
        transforms: 
        - _target_: ${CT}.video.CentralCrop
          size: 224
        - _target_: ${CT}.video.Resize
          h: 192
          w: 192
        - _target_: ${CT}.video.FrameScale
          min: -1.
          max: 1.
          input_range: [0, 255]
        - _target_: ${CT}.video.ToTensor
          keys: [video, gloss]
          dtypes: [float32, default]
    
  train_set: 
    _target_: ${csi}.data.dataset.phoenix14.MyPhoenix14Dataset
    data_root: ${phoenix14_root}
    subset: ${data.subset}
    mode: train
    transform: ${data.transform_train}

  val_set: 
    _target_: ${csi}.data.dataset.phoenix14.MyPhoenix14Dataset
    data_root: ${phoenix14_root}
    subset: ${data.subset}
    mode: dev
    transform: ${data.transform_val}


  train_loader:
    _target_: torch.utils.data.DataLoader
    batch_size: ${batch_size}
    num_workers: ${num_workers}
    pin_memory: ${pin_memory}
    shuffle: true
    dataset: ${data.train_set}
    collate_fn: ${data.collate_fn}

  val_loader:
    _target_: torch.utils.data.DataLoader
    batch_size: ${batch_size}
    num_workers: ${num_workers}
    pin_memory: ${pin_memory}
    dataset: ${data.val_set}
    collate_fn: ${data.collate_fn}
